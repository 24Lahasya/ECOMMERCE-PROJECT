[training@localhost ~]$ hive
Logging initialized using configuration in file:/etc/hive/conf.dist/hive-log4j.properties
Hive history file=/tmp/training/hive_job_log_training_202307201209_1301347269.txt
hive> SELECT * FROM CLICKSTREAMDATA;
OK
1	2023-01-01 10:00:00.0	homepage
1	2023-01-01 10:01:00.0	product_page
2	2023-01-01 10:02:00.0	homepage
2	2023-01-01 10:03:00.0	cart_page
3	2023-01-01 10:05:00.0	homepage
3	2023-01-01 10:06:00.0	product_page
3	2023-01-01 10:07:00.0	cart_page
4	2023-01-01 10:09:00.0	homepage
4	2023-01-01 10:10:00.0	product_page
4	2023-01-01 10:11:00.0	cart_page
4	2023-01-01 10:12:00.0	checkout_page
5	2023-01-01 10:15:00.0	homepage
5	2023-01-01 10:16:00.0	product_page
Time taken: 4.949 seconds
hive> SELECT * FROM CUSTDATA;       
OK
1	John Doe	john.doe@example.com
2	Jane Smith	jane.smith@example.c
3	Robert Johnson	robert.johnson@examp
4	Lisa Brown	lisa.brown@example.c
5	Michael Wilson	michael.wilson@examp
Time taken: 0.114 seconds
hive> SELECT * FROM PURCHASEDATA;
OK
1	2023-01-01 10:05:00.0	100
2	2023-01-01 10:08:00.0	150
3	2023-01-01 10:09:00.0	200
4	2023-01-01 10:13:00.0	120
5	2023-01-01 10:17:00.0	80
Time taken: 0.195 seconds
hive> SELECT * FROM CLICKSTREAMDATA WHERE userID=4;
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_202307201134_0016, Tracking URL = http://0.0.0.0:50030/jobdetails.jsp?jobid=job_202307201134_0016
Kill Command = /usr/lib/hadoop/bin/hadoop job  -Dmapred.job.tracker=0.0.0.0:8021 -kill job_202307201134_0016
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0
2023-07-20 12:14:13,545 Stage-1 map = 0%,  reduce = 0%
2023-07-20 12:14:17,614 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.02 sec
2023-07-20 12:14:18,634 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.02 sec
2023-07-20 12:14:19,644 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.02 sec
2023-07-20 12:14:20,662 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 1.02 sec
MapReduce Total cumulative CPU time: 1 seconds 20 msec
Ended Job = job_202307201134_0016
MapReduce Jobs Launched: 
Job 0: Map: 1   Cumulative CPU: 1.02 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Total MapReduce CPU Time Spent: 1 seconds 20 msec
OK
4	2023-01-01 10:09:00.0	homepage
4	2023-01-01 10:10:00.0	product_page
4	2023-01-01 10:11:00.0	cart_page
4	2023-01-01 10:12:00.0	checkout_page
Time taken: 13.786 seconds
hive> SELECT * , COUNT(*)FROM PURCHASEDATA GROUP BY userID; 
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_202307201134_0003, Tracking URL = http://0.0.0.0:50030/jobdetails.jsp?jobid=job_202307201134_0003
Kill Command = /usr/lib/hadoop/bin/hadoop job  -Dmapred.job.tracker=0.0.0.0:8021 -kill job_202307201134_0003
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2023-07-20 11:46:51,092 Stage-1 map = 0%,  reduce = 0%
2023-07-20 11:46:55,123 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.35 sec
2023-07-20 11:46:56,134 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.35 sec
2023-07-20 11:46:57,150 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.35 sec
2023-07-20 11:46:58,167 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.35 sec
2023-07-20 11:46:59,178 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.35 sec
2023-07-20 11:47:00,324 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.35 sec
2023-07-20 11:47:01,340 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 4.11 sec
2023-07-20 11:47:02,355 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 4.11 sec
2023-07-20 11:47:03,365 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 4.11 sec
2023-07-20 11:47:04,382 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 4.11 sec
MapReduce Total cumulative CPU time: 4 seconds 110 msec
Ended Job = job_202307201134_0003
MapReduce Jobs Launched: 
Job 0: Map: 1  Reduce: 1   Cumulative CPU: 4.11 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Total MapReduce CPU Time Spent: 4 seconds 110 msec
OK
1	1	1
2	1	1
3	1	1
4	1	1
5	1	1
Time taken: 18.649 seconds
hive>  SELECT AVG(amount) FROM PURCHASEDATA;
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_202307201134_0004, Tracking URL = http://0.0.0.0:50030/jobdetails.jsp?jobid=job_202307201134_0004
Kill Command = /usr/lib/hadoop/bin/hadoop job  -Dmapred.job.tracker=0.0.0.0:8021 -kill job_202307201134_0004
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2023-07-20 11:47:42,768 Stage-1 map = 0%,  reduce = 0%
2023-07-20 11:47:46,806 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 0.99 sec
2023-07-20 11:47:47,826 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 0.99 sec
2023-07-20 11:47:48,845 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 0.99 sec
2023-07-20 11:47:49,856 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 0.99 sec
2023-07-20 11:47:50,870 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 0.99 sec
2023-07-20 11:47:51,888 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 2.91 sec
2023-07-20 11:47:52,907 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 2.91 sec
2023-07-20 11:47:53,927 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 2.91 sec
2023-07-20 11:47:54,946 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 2.91 sec
2023-07-20 11:47:55,966 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 2.91 sec
MapReduce Total cumulative CPU time: 2 seconds 910 msec
Ended Job = job_202307201134_0004
MapReduce Jobs Launched: 
Job 0: Map: 1  Reduce: 1   Cumulative CPU: 2.91 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Total MapReduce CPU Time Spent: 2 seconds 910 msec
OK
130.0
Time taken: 18.679 seconds
hive> SELECT MAX(amount) FROM PURCHASEDATA;
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_202307201134_0005, Tracking URL = http://0.0.0.0:50030/jobdetails.jsp?jobid=job_202307201134_0005
Kill Command = /usr/lib/hadoop/bin/hadoop job  -Dmapred.job.tracker=0.0.0.0:8021 -kill job_202307201134_0005
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2023-07-20 11:48:11,219 Stage-1 map = 0%,  reduce = 0%
2023-07-20 11:48:16,292 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.23 sec
2023-07-20 11:48:17,305 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.23 sec
2023-07-20 11:48:18,317 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.23 sec
2023-07-20 11:48:19,336 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.23 sec
2023-07-20 11:48:20,351 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.23 sec
2023-07-20 11:48:21,363 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 3.32 sec
2023-07-20 11:48:22,374 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 3.32 sec
2023-07-20 11:48:23,389 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 3.32 sec
2023-07-20 11:48:24,405 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 3.32 sec
MapReduce Total cumulative CPU time: 3 seconds 320 msec
Ended Job = job_202307201134_0005
MapReduce Jobs Launched: 
Job 0: Map: 1  Reduce: 1   Cumulative CPU: 3.32 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Total MapReduce CPU Time Spent: 3 seconds 320 msec
OK
200
Time taken: 17.953 seconds
hive> SELECT MIN(amount) FROM PURCHASEDATA;
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_202307201134_0006, Tracking URL = http://0.0.0.0:50030/jobdetails.jsp?jobid=job_202307201134_0006
Kill Command = /usr/lib/hadoop/bin/hadoop job  -Dmapred.job.tracker=0.0.0.0:8021 -kill job_202307201134_0006
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2023-07-20 11:48:34,415 Stage-1 map = 0%,  reduce = 0%
2023-07-20 11:48:38,453 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.1 sec
2023-07-20 11:48:39,463 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.1 sec
2023-07-20 11:48:40,472 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.1 sec
2023-07-20 11:48:41,486 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.1 sec
2023-07-20 11:48:42,509 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.1 sec
2023-07-20 11:48:43,524 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 3.53 sec
2023-07-20 11:48:44,554 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 3.53 sec
2023-07-20 11:48:45,567 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 3.53 sec
2023-07-20 11:48:46,583 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 3.53 sec
MapReduce Total cumulative CPU time: 3 seconds 530 msec
Ended Job = job_202307201134_0006
MapReduce Jobs Launched: 
Job 0: Map: 1  Reduce: 1   Cumulative CPU: 3.53 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Total MapReduce CPU Time Spent: 3 seconds 530 msec
OK
80
Time taken: 17.853 seconds
hive> SELECT SUM(amount) FROM PURCHASEDATA;
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_202307201134_0007, Tracking URL = http://0.0.0.0:50030/jobdetails.jsp?jobid=job_202307201134_0007
Kill Command = /usr/lib/hadoop/bin/hadoop job  -Dmapred.job.tracker=0.0.0.0:8021 -kill job_202307201134_0007
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2023-07-20 11:48:56,084 Stage-1 map = 0%,  reduce = 0%
2023-07-20 11:49:00,140 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.08 sec
2023-07-20 11:49:01,166 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.08 sec
2023-07-20 11:49:02,177 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.08 sec
2023-07-20 11:49:03,188 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.08 sec
2023-07-20 11:49:04,197 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 2.84 sec
2023-07-20 11:49:05,207 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 2.84 sec
2023-07-20 11:49:06,236 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 2.84 sec
2023-07-20 11:49:07,253 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 2.84 sec
MapReduce Total cumulative CPU time: 2 seconds 840 msec
Ended Job = job_202307201134_0007
MapReduce Jobs Launched: 
Job 0: Map: 1  Reduce: 1   Cumulative CPU: 2.84 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Total MapReduce CPU Time Spent: 2 seconds 840 msec
OK
650
Time taken: 15.837 seconds
hive> SELECT * FROM CLICKSTREAMDATA INNER JOIN PURCHASEDATA ON CLICKSTREAMDATA.userID = PURCHASEDATA.userID;
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_202307201134_0008, Tracking URL = http://0.0.0.0:50030/jobdetails.jsp?jobid=job_202307201134_0008
Kill Command = /usr/lib/hadoop/bin/hadoop job  -Dmapred.job.tracker=0.0.0.0:8021 -kill job_202307201134_0008
Hadoop job information for Stage-1: number of mappers: 2; number of reducers: 1
2023-07-20 11:49:46,526 Stage-1 map = 0%,  reduce = 0%
2023-07-20 11:49:52,565 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 3.01 sec
2023-07-20 11:49:53,573 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 3.01 sec
2023-07-20 11:49:54,588 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 3.01 sec
2023-07-20 11:49:55,598 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 3.01 sec
2023-07-20 11:49:56,618 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 3.01 sec
2023-07-20 11:49:57,639 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 5.17 sec
2023-07-20 11:49:58,647 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 5.17 sec
2023-07-20 11:49:59,661 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 5.17 sec
2023-07-20 11:50:00,677 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 5.17 sec
MapReduce Total cumulative CPU time: 5 seconds 170 msec
Ended Job = job_202307201134_0008
MapReduce Jobs Launched: 
Job 0: Map: 2  Reduce: 1   Cumulative CPU: 5.17 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Total MapReduce CPU Time Spent: 5 seconds 170 msec
OK
1	2023-01-01 10:01:00.0	product_page	1	2023-01-01 10:05:00.0	100
1	2023-01-01 10:00:00.0	homepage	1	2023-01-01 10:05:00.0	100
2	2023-01-01 10:02:00.0	homepage	2	2023-01-01 10:08:00.0	150
2	2023-01-01 10:03:00.0	cart_page	2	2023-01-01 10:08:00.0	150
3	2023-01-01 10:07:00.0	cart_page	3	2023-01-01 10:09:00.0	200
3	2023-01-01 10:05:00.0	homepage	3	2023-01-01 10:09:00.0	200
3	2023-01-01 10:06:00.0	product_page	3	2023-01-01 10:09:00.0	200
4	2023-01-01 10:09:00.0	homepage	4	2023-01-01 10:13:00.0	120
4	2023-01-01 10:10:00.0	product_page	4	2023-01-01 10:13:00.0	120
4	2023-01-01 10:11:00.0	cart_page	4	2023-01-01 10:13:00.0	120
4	2023-01-01 10:12:00.0	checkout_page	4	2023-01-01 10:13:00.0	120
5	2023-01-01 10:15:00.0	homepage	5	2023-01-01 10:17:00.0	80
5	2023-01-01 10:16:00.0	product_page	5	2023-01-01 10:17:00.0	80
Time taken: 19.048 seconds
hive> SELECT * FROM CUSTDATA INNER JOIN PURCHASEDATA ON CUSTDATA.userID = PURCHASEDATA.userID;
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_202307201134_0009, Tracking URL = http://0.0.0.0:50030/jobdetails.jsp?jobid=job_202307201134_0009
Kill Command = /usr/lib/hadoop/bin/hadoop job  -Dmapred.job.tracker=0.0.0.0:8021 -kill job_202307201134_0009
Hadoop job information for Stage-1: number of mappers: 2; number of reducers: 1
2023-07-20 11:50:18,660 Stage-1 map = 0%,  reduce = 0%
2023-07-20 11:50:23,807 Stage-1 map = 50%,  reduce = 0%, Cumulative CPU 1.15 sec
2023-07-20 11:50:24,818 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.32 sec
2023-07-20 11:50:25,826 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.32 sec
2023-07-20 11:50:26,833 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.32 sec
2023-07-20 11:50:27,841 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.32 sec
2023-07-20 11:50:28,849 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 4.26 sec
2023-07-20 11:50:29,857 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 4.26 sec
2023-07-20 11:50:30,874 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 4.26 sec
MapReduce Total cumulative CPU time: 4 seconds 260 msec
Ended Job = job_202307201134_0009
MapReduce Jobs Launched: 
Job 0: Map: 2  Reduce: 1   Cumulative CPU: 4.26 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Total MapReduce CPU Time Spent: 4 seconds 260 msec
OK
1	John Doe	john.doe@example.com	1	2023-01-01 10:05:00.0	100
2	Jane Smith	jane.smith@example.c	2	2023-01-01 10:08:00.0	150
3	Robert Johnson	robert.johnson@examp	3	2023-01-01 10:09:00.0	200
4	Lisa Brown	lisa.brown@example.c	4	2023-01-01 10:13:00.0	120
5	Michael Wilson	michael.wilson@examp	5	2023-01-01 10:17:00.0	80
Time taken: 17.053 seconds
hive> SELECT * FROM CUSTDATA INNER JOIN CLICKSTREAMDATA ON CUSTDATA.userID = CLICKSTREAMDATA.userID;
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_202307201134_0010, Tracking URL = http://0.0.0.0:50030/jobdetails.jsp?jobid=job_202307201134_0010
Kill Command = /usr/lib/hadoop/bin/hadoop job  -Dmapred.job.tracker=0.0.0.0:8021 -kill job_202307201134_0010
Hadoop job information for Stage-1: number of mappers: 2; number of reducers: 1
2023-07-20 11:51:26,926 Stage-1 map = 0%,  reduce = 0%
2023-07-20 11:51:31,990 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.56 sec
2023-07-20 11:51:33,009 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.56 sec
2023-07-20 11:51:34,022 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.56 sec
2023-07-20 11:51:35,039 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.56 sec
2023-07-20 11:51:36,051 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 4.74 sec
2023-07-20 11:51:37,063 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 4.74 sec
2023-07-20 11:51:38,081 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 4.74 sec
2023-07-20 11:51:39,110 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 4.74 sec
2023-07-20 11:51:40,129 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 4.74 sec
MapReduce Total cumulative CPU time: 4 seconds 740 msec
Ended Job = job_202307201134_0010
MapReduce Jobs Launched: 
Job 0: Map: 2  Reduce: 1   Cumulative CPU: 4.74 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Total MapReduce CPU Time Spent: 4 seconds 740 msec
OK
1	John Doe	john.doe@example.com	1	2023-01-01 10:01:00.0	product_page
1	John Doe	john.doe@example.com	1	2023-01-01 10:00:00.0	homepage
2	Jane Smith	jane.smith@example.c	2	2023-01-01 10:02:00.0	homepage
2	Jane Smith	jane.smith@example.c	2	2023-01-01 10:03:00.0	cart_page
3	Robert Johnson	robert.johnson@examp	3	2023-01-01 10:07:00.0	cart_page
3	Robert Johnson	robert.johnson@examp	3	2023-01-01 10:05:00.0	homepage
3	Robert Johnson	robert.johnson@examp	3	2023-01-01 10:06:00.0	product_page
4	Lisa Brown	lisa.brown@example.c	4	2023-01-01 10:09:00.0	homepage
4	Lisa Brown	lisa.brown@example.c	4	2023-01-01 10:10:00.0	product_page
4	Lisa Brown	lisa.brown@example.c	4	2023-01-01 10:11:00.0	cart_page
4	Lisa Brown	lisa.brown@example.c	4	2023-01-01 10:12:00.0	checkout_page
5	Michael Wilson	michael.wilson@examp	5	2023-01-01 10:15:00.0	homepage
5	Michael Wilson	michael.wilson@examp	5	2023-01-01 10:16:00.0	product_page
Time taken: 17.888 seconds
hive> SELECT * FROM PURCHASEDATA ORDER BY amount DESC;
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_202307201134_0011, Tracking URL = http://0.0.0.0:50030/jobdetails.jsp?jobid=job_202307201134_0011
Kill Command = /usr/lib/hadoop/bin/hadoop job  -Dmapred.job.tracker=0.0.0.0:8021 -kill job_202307201134_0011
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2023-07-20 11:51:53,479 Stage-1 map = 0%,  reduce = 0%
2023-07-20 11:51:58,529 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.08 sec
2023-07-20 11:51:59,537 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.08 sec
2023-07-20 11:52:00,548 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.08 sec
2023-07-20 11:52:01,558 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.08 sec
2023-07-20 11:52:02,569 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 2.76 sec
2023-07-20 11:52:03,579 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 2.76 sec
2023-07-20 11:52:04,587 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 2.76 sec
2023-07-20 11:52:05,596 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 2.76 sec
2023-07-20 11:52:06,617 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 2.76 sec
MapReduce Total cumulative CPU time: 2 seconds 760 msec
Ended Job = job_202307201134_0011
MapReduce Jobs Launched: 
Job 0: Map: 1  Reduce: 1   Cumulative CPU: 2.76 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Total MapReduce CPU Time Spent: 2 seconds 760 msec
OK
3	2023-01-01 10:09:00.0	200
2	2023-01-01 10:08:00.0	150
4	2023-01-01 10:13:00.0	120
1	2023-01-01 10:05:00.0	100
5	2023-01-01 10:17:00.0	80
Time taken: 17.863 seconds
hive> SELECT * FROM PURCHASEDATA ORDER BY amount ASC;
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_202307201134_0012, Tracking URL = http://0.0.0.0:50030/jobdetails.jsp?jobid=job_202307201134_0012
Kill Command = /usr/lib/hadoop/bin/hadoop job  -Dmapred.job.tracker=0.0.0.0:8021 -kill job_202307201134_0012
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2023-07-20 11:52:32,166 Stage-1 map = 0%,  reduce = 0%
2023-07-20 11:52:36,194 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.02 sec
2023-07-20 11:52:37,210 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.02 sec
2023-07-20 11:52:38,227 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.02 sec
2023-07-20 11:52:39,245 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.02 sec
2023-07-20 11:52:40,257 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.02 sec
2023-07-20 11:52:41,301 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 3.28 sec
2023-07-20 11:52:42,314 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 3.28 sec
2023-07-20 11:52:43,324 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 3.28 sec
2023-07-20 11:52:44,337 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 3.28 sec
2023-07-20 11:52:45,359 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 3.28 sec
MapReduce Total cumulative CPU time: 3 seconds 280 msec
Ended Job = job_202307201134_0012
MapReduce Jobs Launched: 
Job 0: Map: 1  Reduce: 1   Cumulative CPU: 3.28 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Total MapReduce CPU Time Spent: 3 seconds 280 msec
OK
5	2023-01-01 10:17:00.0	80
1	2023-01-01 10:05:00.0	100
4	2023-01-01 10:13:00.0	120
2	2023-01-01 10:08:00.0	150
3	2023-01-01 10:09:00.0	200
Time taken: 17.918 seconds
hive> SELECT * FROM CLICKSTREAMDATA WHERE page IS NULL;
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_202307201134_0013, Tracking URL = http://0.0.0.0:50030/jobdetails.jsp?jobid=job_202307201134_0013
Kill Command = /usr/lib/hadoop/bin/hadoop job  -Dmapred.job.tracker=0.0.0.0:8021 -kill job_202307201134_0013
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0
2023-07-20 11:53:09,431 Stage-1 map = 0%,  reduce = 0%
2023-07-20 11:53:13,461 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 0.96 sec
2023-07-20 11:53:14,472 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 0.96 sec
2023-07-20 11:53:15,493 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 0.96 sec
2023-07-20 11:53:16,509 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 0.96 sec
2023-07-20 11:53:17,529 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 0.96 sec
MapReduce Total cumulative CPU time: 960 msec
Ended Job = job_202307201134_0013
MapReduce Jobs Launched: 
Job 0: Map: 1   Cumulative CPU: 0.96 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Total MapReduce CPU Time Spent: 960 msec
OK
Time taken: 12.744 seconds
hive> SELECT * FROM CUSTDATA INNER JOIN PURCHASEDATA ON CUSTDATA.userID = PURCHASEDATA.userID WHERE CUSTDATA.userID <> PURCHASEDATA.userID ;
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_202307201134_0014, Tracking URL = http://0.0.0.0:50030/jobdetails.jsp?jobid=job_202307201134_0014
Kill Command = /usr/lib/hadoop/bin/hadoop job  -Dmapred.job.tracker=0.0.0.0:8021 -kill job_202307201134_0014
Hadoop job information for Stage-1: number of mappers: 2; number of reducers: 1
2023-07-20 11:53:36,618 Stage-1 map = 0%,  reduce = 0%
2023-07-20 11:53:42,688 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.85 sec
2023-07-20 11:53:43,699 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.85 sec
2023-07-20 11:53:44,716 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.85 sec
2023-07-20 11:53:45,728 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.85 sec
2023-07-20 11:53:46,744 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.85 sec
2023-07-20 11:53:47,754 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 4.67 sec
2023-07-20 11:53:48,768 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 4.67 sec
2023-07-20 11:53:49,780 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 4.67 sec
2023-07-20 11:53:50,797 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 4.67 sec
MapReduce Total cumulative CPU time: 4 seconds 670 msec
Ended Job = job_202307201134_0014
MapReduce Jobs Launched: 
Job 0: Map: 2  Reduce: 1   Cumulative CPU: 4.67 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Total MapReduce CPU Time Spent: 4 seconds 670 msec
OK
Time taken: 19.373 seconds
hive> SELECT * FROM PURCHASEDATA WHERE CAST(amount AS INT) IS NULL;
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks is set to 0 since there's no reduce operator
Starting Job = job_202307201134_0015, Tracking URL = http://0.0.0.0:50030/jobdetails.jsp?jobid=job_202307201134_0015
Kill Command = /usr/lib/hadoop/bin/hadoop job  -Dmapred.job.tracker=0.0.0.0:8021 -kill job_202307201134_0015
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0
2023-07-20 11:54:05,877 Stage-1 map = 0%,  reduce = 0%
2023-07-20 11:54:08,904 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 0.94 sec
2023-07-20 11:54:09,912 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 0.94 sec
2023-07-20 11:54:10,918 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 0.94 sec
2023-07-20 11:54:11,930 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 0.94 sec
MapReduce Total cumulative CPU time: 940 msec
Ended Job = job_202307201134_0015
MapReduce Jobs Launched: 
Job 0: Map: 1   Cumulative CPU: 0.94 sec   HDFS Read: 0 HDFS Write: 0 SUCCESS
Total MapReduce CPU Time Spent: 940 msec
OK
Time taken: 11.274 seconds
hive> 
